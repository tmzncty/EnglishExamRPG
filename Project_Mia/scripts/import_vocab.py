"""
import_vocab.py â€” é˜¶æ®µ 2.9: è¯æ±‡æ ¸å¿ƒè£…è½½
==============================================
å°† exam_vocabulary.json çš„ 6000+ è€ƒç ”æ ¸å¿ƒè¯æ±‡å¯¼å…¥ static_content.db dictionary è¡¨ã€‚

æºæ•°æ®ç»“æ„:
  { "word": "ability",
    "meanings": ["n. èƒ½åŠ›ï¼Œèƒ½è€ï¼›æ‰èƒ½"],
    "pos": "n.",
    "sentences": [{ "sentence": "...", "translation": "", "year": 2013,
                     "exam_type": "English I", "section_name": "...",
                     "source_label": "2013 English I Â· ..." }]
  }

ç›®æ ‡å­—æ®µæ˜ å°„:
  word           â†’ word (å»ç©ºæ ¼, å°å†™)
  meanings[]     â†’ meaning (åˆ†å·æ‹¼æ¥)
  pos            â†’ pos
  len(sentences) â†’ frequency (çœŸé¢˜å‡ºç°æ¬¡æ•°)
  sentences[]    â†’ example_sentences (JSON)

Usage:
    python scripts/import_vocab.py
"""

import sqlite3
import json
from pathlib import Path

# --- è·¯å¾„ ---
SRC_JSON = Path(r"F:\sanity_check_avg\VocabWeb\data\exam_vocabulary.json")
DB_PATH  = Path(__file__).resolve().parent.parent / "backend" / "data" / "static_content.db"

# --- é¢œè‰² ---
RED    = "\033[91m"
GREEN  = "\033[92m"
YELLOW = "\033[93m"
CYAN   = "\033[96m"
BOLD   = "\033[1m"
RESET  = "\033[0m"


def main():
    print(f"\n{BOLD}{CYAN}{'='*60}")
    print(f"  ğŸ“– Project_Mia â€” Vocabulary Import")
    print(f"{'='*60}{RESET}\n")

    # --- è¯»å–æº JSON ---
    print(f"  Reading: {SRC_JSON}")
    if not SRC_JSON.exists():
        print(f"  {RED}[ERROR] Source file not found!{RESET}")
        return

    with open(SRC_JSON, "r", encoding="utf-8") as f:
        vocab_list = json.load(f)

    total = len(vocab_list)
    print(f"  Found: {total} words\n")

    # --- è¿æ¥æ•°æ®åº“ ---
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()

    # ç¡®ä¿è¡¨å­˜åœ¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS dictionary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            word VARCHAR(50) UNIQUE NOT NULL,
            meaning TEXT NOT NULL,
            pos VARCHAR(20),
            frequency INTEGER DEFAULT 0,
            example_sentences TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    cursor.execute("CREATE INDEX IF NOT EXISTS ix_dictionary_word ON dictionary(word)")

    # --- æ‰¹é‡å¯¼å…¥ ---
    inserted = 0
    updated = 0
    errors = 0
    skipped_empty = 0

    batch_size = 500
    batch_count = 0

    for i, item in enumerate(vocab_list):
        try:
            word = item.get("word", "").strip()
            if not word:
                skipped_empty += 1
                continue

            # é‡Šä¹‰: æ•°ç»„æ‹¼æ¥ä¸ºåˆ†å·åˆ†éš”
            meanings_raw = item.get("meanings", [])
            if isinstance(meanings_raw, list):
                meaning = "; ".join(m.strip() for m in meanings_raw if m.strip())
            else:
                meaning = str(meanings_raw).strip()

            if not meaning:
                meaning = "[æœªçŸ¥é‡Šä¹‰]"

            pos = item.get("pos", "").strip() or None

            # ä¾‹å¥ (ä¿ç•™å®Œæ•´ç»“æ„ç”¨äºè·¨å¹´ä»½è”åŠ¨)
            sentences = item.get("sentences", [])
            frequency = len(sentences)

            # ç®€åŒ–ä¾‹å¥: åªä¿ç•™ sentence, year, source_label
            clean_sentences = []
            for s in sentences:
                clean_sentences.append({
                    "en": s.get("sentence", ""),
                    "cn": s.get("translation", ""),
                    "year": s.get("year"),
                    "source": s.get("source_label", ""),
                })

            sentences_json = json.dumps(clean_sentences, ensure_ascii=False) if clean_sentences else None

            # Upsert
            cursor.execute("""
                INSERT INTO dictionary (word, meaning, pos, frequency, example_sentences)
                VALUES (?, ?, ?, ?, ?)
                ON CONFLICT(word) DO UPDATE SET
                    meaning = excluded.meaning,
                    pos = excluded.pos,
                    frequency = excluded.frequency,
                    example_sentences = excluded.example_sentences
            """, (word, meaning, pos, frequency, sentences_json))

            if cursor.rowcount > 0:
                inserted += 1
            else:
                updated += 1

            batch_count += 1
            if batch_count >= batch_size:
                conn.commit()
                batch_count = 0

            # è¿›åº¦æ¡ (æ¯500ä¸ªæ‰“å°)
            if (i + 1) % 1000 == 0 or (i + 1) == total:
                pct = (i + 1) / total * 100
                bar_len = 30
                filled = int(bar_len * (i + 1) / total)
                bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
                print(f"\r  [{bar}] {pct:5.1f}% ({i+1}/{total})", end="", flush=True)

        except Exception as e:
            errors += 1
            if errors <= 5:
                print(f"\n  {RED}[ERROR] {word}: {e}{RESET}")

    conn.commit()
    conn.close()
    print()

    # --- æŠ¥å‘Š ---
    print(f"\n{BOLD}  ğŸ“Š Import Summary{RESET}")
    print(f"  {'â”€'*40}")
    print(f"  Total processed:  {total}")
    print(f"  Inserted/Updated: {GREEN}{inserted}{RESET}")
    print(f"  Skipped (empty):  {skipped_empty}")
    print(f"  Errors:           {RED if errors else GREEN}{errors}{RESET}")

    if errors == 0:
        print(f"\n  {GREEN}{BOLD}âœ… Vocabulary import complete!{RESET}\n")
    else:
        print(f"\n  {YELLOW}âš  Import completed with {errors} errors.{RESET}\n")


if __name__ == "__main__":
    main()
