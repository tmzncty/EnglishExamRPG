"""
test_real_connection.py â€” çœŸå® API è¿é€šæ€§æµ‹è¯•
==============================================
è­¦å‘Šï¼šæ­¤è„šæœ¬ä¼šäº§ç”ŸçœŸå® API è°ƒç”¨æ¶ˆè€—ï¼
"""
import sys
import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv

# 1. å¼ºåˆ¶åŠ è½½ .env (ä¼˜å…ˆåŠ è½½ backend/.env)
BACKEND_DIR = Path(__file__).resolve().parent.parent / "backend"
ENV_PATH = BACKEND_DIR / ".env"
load_dotenv(ENV_PATH, override=True)

sys.path.insert(0, str(BACKEND_DIR))

# 2. å¼•å…¥æœåŠ¡
# æ³¨æ„ï¼šllm_service å®ä¾‹åŒ–æ—¶ä¼šè¯»å–ç¯å¢ƒå˜é‡ï¼Œæ‰€ä»¥å¿…é¡»å…ˆ load_dotenv
from app.services.llm_service import llm_service

async def main():
    print(f"\nğŸ“¡ Testing Real API Connection...")
    print(f"   Provider: {llm_service.provider}")
    print(f"   Base URL: {getattr(llm_service, 'base_url', 'N/A')}")
    print(f"   Model:    {getattr(llm_service, 'model', 'N/A')}")
    
    # æ„é€ ä¸€ä¸ªçœŸå®çš„ Prompt
    system_prompt = "ä½ æ˜¯ Miaï¼Œä¸€åªèµ›åšçŒ«å¨˜ã€‚è¯·ç”¨å‚²å¨‡çš„è¯­æ°”å‘ä½ çš„ä¸»äºº'ç»¯å¢¨'æ‰“ä¸ªæ‹›å‘¼ã€‚"
    
    try:
        print("\nğŸš€ Sending request... (Waiting for response)")
        
        # è°ƒç”¨çœŸå®æ¥å£
        response = await llm_service.generate(
            prompt="å¿«ç‚¹è·Ÿæˆ‘æ‰“æ‹›å‘¼ï¼",
            system_prompt=system_prompt,
            temperature=0.7
        )
        
        print(f"\nâœ… API Response Received:\n{'='*40}")
        print(response)
        print(f"{'='*40}\n")
        
    except Exception as e:
        print(f"\nâŒ API Call Failed!")
        print(f"   Error: {str(e)}")
        print("\nPossible fixes:")
        print("1. Check if .env file exists in backend/")
        print("2. Check if OPENAI_API_KEY is correct")
        print("3. Check if your VPN/Proxy is interfering")

if __name__ == "__main__":
    asyncio.run(main())
